# https://betterexplained.com/articles/intuitive-convolution/

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import math
from sklearn.semi_supervised import SelfTrainingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, export_text
from sklearn.naive_bayes import GaussianNB
from sklearn.mixture import GaussianMixture
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.manifold import Isomap
from metric_learn import LMNN
from hmmlearn import hmm
from sklearn.neural_network import BernoulliRBM
from sklearn.metrics import accuracy_score

# 生成示例数据集
df = pd.read_excel('origin.xlsx', header=None)
tensor = torch.tensor(df.to_numpy(), dtype=torch.float)
# 创建一个形状为 (batch_size, in_channels, sequence*sequence) 的输入张量
tensor = tensor.T
tensorseq = torch.stack((tensor, tensor + 1, tensor + 2, tensor + 3, tensor + 4, tensor + 5), axis=-1)
TensorSeq = torch.stack((tensorseq, tensorseq + 1, tensorseq + 2, tensorseq + 3, tensorseq + 4, tensorseq + 5), axis=-1)

# nn.Model
# Conv: batch_size, in_channels, sequence...
conv1d = nn.Conv1d(in_channels=TensorSeq.shape[1], out_channels=TensorSeq.shape[1], kernel_size=2, stride=1)
conv2d = nn.Conv2d(in_channels=TensorSeq.shape[1], out_channels=TensorSeq.shape[1], kernel_size=[2, 3], stride=1)
# Pool
max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1)
max_pool2d = nn.MaxPool2d(kernel_size=[2, 3], stride=1)
avg_pool2d = nn.AvgPool2d(kernel_size=[2, 3], stride=1)

# Linear: batch_size, ..., sequence
linear = nn.Linear(in_features=tensorseq.shape[-1], out_features=tensorseq.shape[-1])
# Norm: batch_size, in_channels, sequences...
norm1d = nn.BatchNorm1d(num_features=tensorseq.shape[1])
norm2d = nn.BatchNorm2d(num_features=tensorseq.shape[1])

# RNN: batch_size, sequence, in_channels
# input_size 是in_channels
# hidden_size 是隐藏层的特征数量
# num_layers 是RNN的层数
rnn = nn.RNN(input_size=tensorseq.shape[1], hidden_size=tensorseq.shape[1], num_layers=tensorseq.shape[-1],
             batch_first=True)
# GRU
gru = nn.GRU(input_size=tensorseq.shape[1], hidden_size=tensorseq.shape[1], num_layers=tensorseq.shape[-1],
             batch_first=True)
# LSTM
lstm = nn.LSTM(input_size=tensorseq.shape[1], hidden_size=tensorseq.shape[1], num_layers=tensorseq.shape[-1],
               batch_first=True)

# Attention: batch_size, sequence, in_channels
multihead_attn = nn.MultiheadAttention(embed_dim=tensorseq.shape[1], num_heads=tensorseq.shape[1], batch_first=True)

# Embedding: batch_size, sequence
# num_embeddings 是factor数量
# embedding_dim 是向量化维度
embedding = nn.Embedding(num_embeddings=int(torch.max(tensorseq)), embedding_dim=int(torch.max(tensorseq)))

# logist_binary
relu = nn.ReLU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
dropout = nn.Dropout(p=0.5)

# classifier
# SVM
base_svc = SVC(probability=True, kernel='rbf', gamma='scale', random_state=0)
SVM_Classifier = SelfTrainingClassifier(base_svc, criterion='k_best', k_best=math.ceil(0.1 * tensorseq.shape[0]))
# DecisionTree
DecisionTree_Classifier = DecisionTreeClassifier(criterion='gini', max_depth=tensorseq.shape[-1], random_state=0)
# NB
NB_Classifier = GaussianNB()
# EM_GaussianMixture
EM_Classifier = GaussianMixture(n_components=2, random_state=0)
# AdaBoost
AdaBoost_Classifier = AdaBoostClassifier(n_estimators=tensorseq.shape[-1], random_state=0)
# Bagged Decision Trees
BagTrees_Classifier = BaggingClassifier(estimator=DecisionTree_Classifier, n_estimators=tensorseq.shape[-1],
                                        random_state=0)
# RF
RF_Classifier = RandomForestClassifier(n_estimators=tensorseq.shape[-1], random_state=0)
# kNN
kNN_Classifier = KNeighborsClassifier(n_neighbors=2)

# Manifold
isomap_Manifold = Isomap(n_components=2, n_neighbors=2)

# Metric
LMNN_Metric = LMNN(k=2, learn_rate=1e-6)

# States
# GaussianHMM
GaussianHMM = hmm.GaussianHMM(n_components=2, covariance_type="diag", n_iter=tensorseq.shape[-1], random_state=0)
# BernoulliRBM
GB_RBM = BernoulliRBM(n_components=2, learning_rate=0.01, n_iter=tensorseq.shape[-1], random_state=0)

# criterion: batch_size, out_channels
criterion = nn.CrossEntropyLoss()
criterion = nn.MSELoss()
criterion = nn.BCELoss()

# forward
# Conv
output_tensor = conv1d(tensorseq)
output_tensor = conv2d(TensorSeq)
# Pool
output_tensor = max_pool1d(tensorseq)
output_tensor = max_pool2d(TensorSeq)
output_tensor = avg_pool2d(TensorSeq)

# Linear
output_tensor = linear(tensorseq)
# Norm
output_tensor = norm1d(tensorseq)
output_tensor = norm2d(TensorSeq)

# RNN
# 初始化状态h0: num_layers, batch_size, hidden_size
h0 = torch.zeros(tensorseq.shape[-1], tensorseq.shape[0], tensorseq.shape[1])
output_tensor, hn = rnn(tensorseq.permute(0, 2, 1), h0)
output_tensor = output_tensor.permute(0, 2, 1)
hn = hn.permute(1, 2, 0)
# GRU
output_tensor, hn = gru(tensorseq.permute(0, 2, 1), h0)
output_tensor = output_tensor.permute(0, 2, 1)
hn = hn.permute(1, 2, 0)
# LSTM
c0 = torch.zeros(tensorseq.shape[-1], tensorseq.shape[0], tensorseq.shape[1])
output_tensor, (hn, cn) = lstm(tensorseq.permute(0, 2, 1), (h0, c0))
output_tensor = output_tensor.permute(0, 2, 1)
hn = hn.permute(1, 2, 0)
cn = cn.permute(1, 2, 0)

# Attention
output_tensor, attn_output_weights = multihead_attn(tensorseq.permute(0, 2, 1), tensorseq.permute(0, 2, 1),
                                                    tensorseq.permute(0, 2, 1))
output_tensor = output_tensor.permute(0, 2, 1)

# Embedding
index_to_word = {idx: word for idx, word in enumerate(range(int(torch.max(tensorseq))))}
output_tensor = embedding(tensorseq[0][0].int())
# 将词向量还原为索引
cosine_sim = F.cosine_similarity(output_tensor.unsqueeze(2), embedding.weight.T.unsqueeze(0).unsqueeze(0), dim=-1)
# 找到相似度最高的索引
_, closest_indices = cosine_sim.max(dim=-1)
# 将索引还原为词
output_words = [[index_to_word[idx.item()] for idx in sequence] for sequence in closest_indices]

# logist_binary
output_relu = relu(output_tensor)
output_sigmoid = sigmoid(output_tensor)
output_tanh = tanh(output_tensor)
output_leaky_relu = leaky_relu(output_tensor)
output_dropout = dropout(output_tensor)

# classifier
# SVM
# 将训练集中的一部分标签设置为 -1 表示未标记数据
X_train = tensorseq.view(tensorseq.shape[0], -1)
y_train = np.array([1, 1, 0, 0])
y_train_unlabeled = np.array([1, 1, 0, -1])
# SVM
# partial label
SVM_Classifier.fit(X_train, y_train_unlabeled)
y_pred = SVM_Classifier.predict(X_train)
# DecisionTree
DecisionTree_Classifier.fit(X_train, y_train)
y_pred = DecisionTree_Classifier.predict(X_train)
rules = export_text(DecisionTree_Classifier)
# NB
NB_Classifier.fit(X_train, y_train)
y_pred = NB_Classifier.predict(X_train)
# EM
# no label
EM_Classifier.fit(X_train)
y_pred = EM_Classifier.predict(X_train)
# AdaBoost
AdaBoost_Classifier.fit(X_train, y_train)
y_pred = AdaBoost_Classifier.predict(X_train)
# Bagged Decision Trees
BagTrees_Classifier.fit(X_train, y_train)
y_pred = BagTrees_Classifier.predict(X_train)
# RF
RF_Classifier.fit(X_train, y_train)
y_pred = RF_Classifier.predict(X_train)
# k-Nearest Neighbors
kNN_Classifier.fit(X_train, y_train)
y_pred = kNN_Classifier.predict(X_train)

# Manifold: isomap
# no label
y_pred = isomap_Manifold.fit_transform(X_train)

# Metric: LMNN transform
LMNN_Metric.fit(X_train, y_train)
X_train_lmnn = LMNN_Metric.transform(X_train)
# 使用 k-NN 进行分类
kNN_Classifier.fit(X_train_lmnn, y_train)
y_pred = kNN_Classifier.predict(X_train)

# HMM
# no label
GaussianHMM.fit(X_train)
hidden_states = GaussianHMM.predict(X_train)

# RBM
GB_RBM.fit(X_train)
hidden_layer = GB_RBM.transform(X_train)
print("Hidden layer activations:")
print(hidden_layer)
print("Components (weights):")
print(GB_RBM.components_)
print("Intercepts of visible units:")
print(GB_RBM.intercept_visible_)
print("Intercepts of hidden units:")
print(GB_RBM.intercept_hidden_)

accuracy = accuracy_score(y_train, y_pred)

# loss : input_tensor 的形状为 (batch_size, out_channels)
# CrossEntropyLoss: target_tensor 的形状为 (batch_size)
# BCELoss, MSELoss: target_tensor 的形状为 (batch_size, out_channels)
loss = criterion(tensorseq, output_tensor)

print(output_tensor.shape)
